# Count Buffy ğŸ§›â€â™€ï¸

## Getting Started

### Requirements

Install [Docker](https://docs.docker.com/get-started/get-docker/)

#### Update submodules

```shell
git submodule update --init --recursive
```

#### Pull ğŸ§›â€â™€ï¸ Image

```shell
docker compose pull
```

### Hello World Example

The `examples` directory includes a simple example of using ğŸ§›â€â™€ï¸ to
model a rate-limiter scheduler and verify certain workload and queries.

```shell
docker compose run --rm buffy hello-world
```

### Run Experiments

For performance evaluation of ğŸ§›â€â™€ï¸ we used the 4 case studies used
in FPerf.
For each case study, we run the FPerf search for various buffers sizes
and recorded all workloads generated during the search.
Then we checked these workloads in ğŸ§›â€â™€ï¸ to ensure we get the
same SAT/UNSAT result and compare the time of each call to the verification engine.
This way we can compare the efficiency of ğŸ§›â€â™€ï¸ against FPerf.

Data for our experimental results include:

- Workloads generated by FPerf
- Output logs of ğŸ§›â€â™€ï¸ that includes verification time of each workload
- Charts to compare the average verification time of workloads in ğŸ§›â€â™€ï¸ and FPerf

To reproduce the result claimed in the paper, we proceed in 4 steps:

1. Generate charts from the existing output of ğŸ§›â€â™€ï¸ and FPerf without running either ğŸ§›â€â™€ï¸ or FPerf
2. Use the existing output of FPerf and generate the output of ğŸ§›â€â™€ï¸
3. Check existing output of FPerf with the FPerf itself
4. Generate the output of FPerf

This way we can gradually increase the complexity of reproducing the results. 
Moreover, steps 3 and 4 are significantly more time-consuming, and require a third-party
implementation (FPerf), so we decided to separate these step with the steps that are 
solely depend on ğŸ§›â€â™€ï¸.

### 1- Generate Charts with Existing Outputs

- `data/sub_logs` includes the output of ğŸ§›â€â™€ï¸ (logs generated for submission)
- `data/sub_wls` includes the output of FPerf that we already

```shell
docker compose run --rm -e BUFFY_LOGS_DIR=data/sub_logs buffy draw_all_charts.sh
```

Charts are saved in the `data/charts` directory.

> Here we are using previously generated data used for the submission (`data/sub_logs` and `data/sub_wls`)

### 2- Verify FPerf Workloads in ğŸ§›â€â™€ï¸

- `data/logs` includes the output of ğŸ§›â€â™€ï¸ (new logs that you generate)
  generated and stored here

Clear existing log files:

```shell
rm -rf data/logs data/charts
```

Verify all workloads in ğŸ§›â€â™€ï¸:

```shell
docker compose run --rm buffy run_all_experiments.sh
```

draw the charts again:

```shell
docker compose run --rm buffy draw_all_charts.sh
```

### 3- Check Existing Workloads in FPerf

Check workloads in FPerf:

```shell
docker compose run --rm buffy check_all_workloads_with_fperf.sh
```

### 4- Generate Workloads in FPerf

Generate workloads of FPerf during the search:

```shell
docker compose run --rm buffy run_all_fperf.sh
```

FPerf new workloads are saved into the `data/new_wls` directory.

Verify newly generated workloads in ğŸ§›â€â™€ï¸:

```shell
docker compose run --rm -e BUFFY_WLS_DIR=data/new_wls buffy run_all_experiments.sh
```

draw the charts again:

```shell
docker compose run --rm -e BUFFY_WLS_DIR=data/new_wls buffy draw_all_charts.sh
```

