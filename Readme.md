# Count Buffy ğŸ§›â€â™€ï¸

Count Buffy (ğŸ§›â€â™€ï¸) is a verification tool for performance verification of packet
schedulers in the network.

Terminology:
- Buffer: A fifo queue of packets with finite capacity
- Packet Stream: A sequence of packets 
- Packet Scheduler: A processing unit that reads packet from a set of input Buffers and
produces a set of output packet streams
- Query: A predicate over output packet stream 
- Workload: A predicate over input packet stream

On a high level, input and output of a packet scheduler is a set of packet streams.
Input packet streams are fed into Buffers and scheduler reads the buffered packets, and don't
directly have access to the input stream.
Scheduler the processes the packets and produces output stream accordingly.
Workload/Query is an abstract representation of a set of inputs/outputs to/from the scheduler.

Queries are performance properties, i.e., they specify performance related properties of 
the output like throughput.

The goal of verification task is to verify whenever we fed traffic consistent with the workload
into the scheduler, the output of the scheduler satisfies the Query.

For instance, assume that we have a rate limiter that limits the output traffic rate to 
1 packet per two timesteps. 
Our goal is to verify that even if we feed consistent traffic of 1 packet every timestep into 
the network, we see at most one packet every two timesteps in the output.

Without verification, we need to manually construct some input traffic with these specification,
feed them into the network, and then check if the output is rate-limited.

With verification, we represent all possible traffic that with a few lines of specification, and
use the verification tool to verify that the output of all of such traffic is satisfies the query.

### Okay! That's typical formal methodsy stuff! What is ğŸ§›â€â™€ï¸'s novelty?

Efficient verification of schedulers with arbitrary large buffer sizes.

The main novelty of ğŸ§›â€â™€ï¸modeling is that it allows analysis when considering
buffer sizes of arbitrary large sizes. 
The previous tool for performance verification, only capable of very small and unrealistic
buffer sizes (â‰¤1K).

## Experiments

### FPerf

FPerf uses a certain grammar to specify workloads. 
Using a CEGIS style search through the state space of the possible
workloads allowed by the grammar, FPerf finds a Workload that
always results in the Query.

FPerf works by using a search loop, inside the loop FPerf generates
a new random workload, and then verifies whether workload always 
result in the query.
To verify most of the workloads generated during the search, FPerf calls
a verification engine.

We show that using ğŸ§›â€â™€ï¸gives significantly better verification time
than the scheduler model used in FPerf when we increase the buffer size.
We showed that, size of buffer has negligible effect on the verification
time in ğŸ§›â€â™€ï¸while FPerf verification time increases by increasing the buffer
size and becomes intractable for larger and realistic buffer sizes.

### How the experiments work?

In each test-case, FPerf generates a set of workloads and verifies each using
Z3.
We record those workloads and verify them in ğŸ§›â€â™€ï¸, simulating the situation where
we keep the FPerf's search algorithm, while replacing the formal model of network
as designed in ğŸ§›â€â™€ï¸.

We compare the average verification time of individual calls to the verification engine
for each buffer size.


## Getting Started

### Requirements

Install [Docker](https://docs.docker.com/get-started/get-docker/)


#### Clone 

```shell
git clone https://github.com/all-things-networking/count-buffy.git
cd count-buffy
git submodule update --init --recursive
```

#### Pull ğŸ§›â€â™€ï¸ Image

```shell
docker compose pull
```

### Hello World Example

The `examples` directory includes a simple example of using ğŸ§›â€â™€ï¸ to
model a rate-limiter scheduler and verify certain workload and queries.

```shell
docker compose run --rm buffy hello-world
```

The output shows a sequence of input and output traffic.
Each sequence shows the number of packets at each time step.
In the output traffic, there should be at most on packet 
within each two subsequent time steps.


### Run Experiments

For performance evaluation of ğŸ§›â€â™€ï¸ we used the 4 case studies used
in FPerf.
For each case study, we run the FPerf search for various buffers sizes
and recorded all workloads generated during the search.
Then we checked these workloads in ğŸ§›â€â™€ï¸ to ensure we get the
same SAT/UNSAT result and compare the time of each call to the verification engine.
This way we can compare the efficiency of ğŸ§›â€â™€ï¸ against FPerf.

Data for our experimental results include:

- Workloads generated by FPerf
- Output logs of ğŸ§›â€â™€ï¸ that includes verification time of each workload
- plots to compare the average verification time of workloads in ğŸ§›â€â™€ï¸ and FPerf

To reproduce the result claimed in the paper, we proceed in 4 steps:

1. Generate plots from the existing output of ğŸ§›â€â™€ï¸ and FPerf without running either ğŸ§›â€â™€ï¸ or FPerf
2. Use the existing output of FPerf and generate the output of ğŸ§›â€â™€ï¸
3. Check existing output of FPerf with the FPerf itself
4. Generate the output of FPerf

This way we can gradually increase the complexity of reproducing the results. 
Moreover, steps 3 and 4 are significantly more time-consuming, and require a third-party
implementation (FPerf), so we decided to separate these step with the steps that are 
solely depend on ğŸ§›â€â™€ï¸.

### 1- Generate plots with Existing Outputs

- `data/sub_logs` includes the output of ğŸ§›â€â™€ï¸ (logs generated for submission)
- `data/sub_wls` includes the output of FPerf that we already

```shell
docker compose run --rm -e BUFFY_LOGS_DIR=data/sub_logs buffy draw_all_plots.sh
```

plots are saved in the `data/plots` directory.

> Here we are using previously generated data used for the submission (`data/sub_logs` and `data/sub_wls`)

### 2- Verify FPerf Workloads in ğŸ§›â€â™€ï¸

- `data/logs` includes the output of ğŸ§›â€â™€ï¸ (new logs that you generate)
  generated and stored here

Clear existing log files:

```shell
rm -rf data/logs data/plots
```

Verify all workloads in ğŸ§›â€â™€ï¸:

```shell
docker compose run --rm buffy run_all_experiments.sh
```

draw the plots again:

```shell
docker compose run --rm buffy draw_all_plots.sh
```

### 3- Check Existing Workloads in FPerf

Check workloads in FPerf:

```shell
docker compose run --rm buffy check_all_workloads_with_fperf.sh
```

### 4- Generate Workloads in FPerf

Generate workloads of FPerf during the search:

```shell
docker compose run --rm buffy run_all_fperf.sh
```

FPerf new workloads are saved into the `data/new_wls` directory.

Verify newly generated workloads in ğŸ§›â€â™€ï¸:

```shell
docker compose run --rm -e BUFFY_WLS_DIR=data/new_wls buffy run_all_experiments.sh
```

draw the plots again:

```shell
docker compose run --rm -e BUFFY_WLS_DIR=data/new_wls buffy draw_all_plots.sh
```
